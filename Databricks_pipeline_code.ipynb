{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f2b38dc-2774-47e4-a32f-27f927f65485",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, length, avg\n",
    "from pyspark.sql.types import StringType\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"DataProcessing\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2648f19-2348-4ad3-9280-82e6eebf2395",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def item_least_rating(data):\n",
    "    least_rated_item = data.orderBy(\"rating\").first().asDict() if data.count() > 0 else None\n",
    "    return least_rated_item\n",
    "\n",
    "def item_most_rating(data):\n",
    "    most_rated_item = data.orderBy(col(\"rating\").desc()).first().asDict() if data.count() > 0 else None\n",
    "    return most_rated_item\n",
    "\n",
    "def item_longest_review(data):\n",
    "    longest_review_item = data.orderBy(length(\"review_text\").desc()).first().asDict() if data.count() > 0 else None\n",
    "    return longest_review_item\n",
    "\n",
    "def transform_date(data):\n",
    "    data = data.withColumn(\"new_date\", data[\"old_date_col\"].cast(StringType()))\n",
    "    return data\n",
    "\n",
    "def desired_dataframe_operation(data):\n",
    "    avg_rating_per_item = data.groupBy(\"item_id\").agg(avg(\"rating\").alias(\"avg_rating\"))\n",
    "    return avg_rating_per_item.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ebf459f1-90ce-4176-9bd5-e7c90e164164",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n  File \"/databricks/python/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3378, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<command-4091231503070435>\", line 40, in <module>\n    main()\n  File \"<command-4091231503070435>\", line 4, in main\n    least_rated_item = item_least_rating(sample_data)\n  File \"<command-4091231503070434>\", line 12, in item_least_rating\n    least_rated_item = data.orderBy(\"rating\").first().asDict() if data.count() > 0 else None\n  File \"/databricks/spark/python/pyspark/instrumentation_utils.py\", line 48, in wrapper\n    res = func(*args, **kwargs)\n  File \"/databricks/spark/python/pyspark/sql/dataframe.py\", line 2613, in sort\n    jdf = self._jdf.sort(self._sort_cols(cols, kwargs))\n  File \"/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\", line 1321, in __call__\n    return_value = get_return_value(\n  File \"/databricks/spark/python/pyspark/errors/exceptions.py\", line 234, in deco\n    raise converted from None\npyspark.errors.exceptions.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `rating` cannot be resolved. Did you mean one of the following? [`reviews`, `user_id`, `user_url`, `_corrupt_record`].;\n'Sort ['rating ASC NULLS FIRST], true\n+- Relation [_corrupt_record#37,reviews#38,user_id#39,user_url#40] json\n\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/databricks/python/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 1997, in showtraceback\n    stb = self.InteractiveTB.structured_traceback(\n  File \"/databricks/python/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1112, in structured_traceback\n    return FormattedTB.structured_traceback(\n  File \"/databricks/python/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1006, in structured_traceback\n    return VerboseTB.structured_traceback(\n  File \"/databricks/python/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 859, in structured_traceback\n    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n  File \"/databricks/python/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 812, in format_exception_as_a_whole\n    frames.append(self.format_record(r))\n  File \"/databricks/python/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 730, in format_record\n    result += ''.join(_format_traceback_lines(frame_info.lines, Colors, self.has_colors, lvals))\n  File \"/databricks/python/lib/python3.9/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n    value = obj.__dict__[self.func.__name__] = self.func(obj)\n  File \"/databricks/python/lib/python3.9/site-packages/stack_data/core.py\", line 698, in lines\n    pieces = self.included_pieces\n  File \"/databricks/python/lib/python3.9/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n    value = obj.__dict__[self.func.__name__] = self.func(obj)\n  File \"/databricks/python/lib/python3.9/site-packages/stack_data/core.py\", line 649, in included_pieces\n    pos = scope_pieces.index(self.executing_piece)\n  File \"/databricks/python/lib/python3.9/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n    value = obj.__dict__[self.func.__name__] = self.func(obj)\n  File \"/databricks/python/lib/python3.9/site-packages/stack_data/core.py\", line 628, in executing_piece\n    return only(\n  File \"/databricks/python/lib/python3.9/site-packages/executing/executing.py\", line 164, in only\n    raise NotOneValueFound('Expected one value, found 0')\nexecuting.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "<span class='ansi-red-fg'>AnalysisException</span>: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `rating` cannot be resolved. Did you mean one of the following? [`reviews`, `user_id`, `user_url`, `_corrupt_record`].;\n'Sort ['rating ASC NULLS FIRST], true\n+- Relation [_corrupt_record#37,reviews#38,user_id#39,user_url#40] json\n",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "def main():\n",
    "    sample_data = spark.read.format(\"json\").load(\"dbfs:/FileStore/shared_uploads/wesleyjohnjayakumar@gmail.com/australian_user_reviews.json\")\n",
    "\n",
    "    least_rated_item = item_least_rating(sample_data)\n",
    "    if least_rated_item:\n",
    "        print(\"Item with the least rating:\")\n",
    "        print(least_rated_item)\n",
    "    else:\n",
    "        print(\"No data found for item with the least rating.\")\n",
    "    \n",
    "    #--------------------------------------------------------------------------\n",
    "\n",
    "    most_rated_item = item_most_rating(sample_data)\n",
    "    if most_rated_item:\n",
    "        print(\"\\nItem with the most rating:\")\n",
    "        print(most_rated_item)\n",
    "    else:\n",
    "        print(\"No data found for item with the most rating.\")\n",
    "    #--------------------------------------------------------------------------\n",
    "\n",
    "    longest_review_item = item_longest_review(sample_data)\n",
    "    if longest_review_item:\n",
    "        print(\"\\nItem with the longest review:\")\n",
    "        print(longest_review_item)\n",
    "    else:\n",
    "        print(\"No data found for item with the longest review.\")\n",
    "    #--------------------------------------------------------------------------\n",
    "\n",
    "    transformed_data = transform_date(sample_data)\n",
    "\n",
    "    desired_operation_result = desired_dataframe_operation(transformed_data)\n",
    "    if desired_operation_result:\n",
    "        print(\"\\nDesired DataFrame operation result: Average Rating per Item\")\n",
    "        for row in desired_operation_result:\n",
    "            print(row)\n",
    "    else:\n",
    "        print(\"No data found for desired DataFrame operation.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aaecf163-33de-4037-a71a-ca0c03847919",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "    #--------------------------------------------------------------------------\n",
    "    savefile_path = \"/..../output_data.parquet\"  # Replace  with your desired directory name\n",
    "\n",
    "    # Save DataFrame to Parquet file in Databricks File System (DBFS)\n",
    "    transformed_data.write.mode(\"overwrite\").parquet(savefile_path)\n",
    "\n",
    "    #--------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6d959517-a1b0-4c91-9fd7-dbd258075f2b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"ParquetToMySQL\").getOrCreate()\n",
    "\n",
    "# Read the Parquet file into a PySpark DataFrame\n",
    "file_path = \"path_to_your_parquet_file\"  # Replace with the actual file path\n",
    "loaded_df = spark.read.parquet(file_path)\n",
    "\n",
    "# Display the loaded DataFrame\n",
    "loaded_df.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "33b79772-d90b-45c2-9b3c-9fc7902bfb34",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Define the MySQL properties\n",
    "db_name = 'mydb'\n",
    "host = 'localhost'\n",
    "user = 'root'\n",
    "password = 'password'\n",
    "table_name = 'table_name'\n",
    "\n",
    "# Write the DataFrame to the MySQL table\n",
    "loaded_df.write.format(\"jdbc\").option(\"url\", f\"jdbc:mysql://{host}/{db_name}\") \\\n",
    "    .option(\"dbtable\", table_name) \\\n",
    "    .option(\"user\", user) \\\n",
    "    .option(\"password\", password) \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()\n",
    "\n",
    "# Stop the SparkSession\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Databricks_pipeline",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
