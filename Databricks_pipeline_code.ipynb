{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f2b38dc-2774-47e4-a32f-27f927f65485",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, length, avg\n",
    "from pyspark.sql.types import StringType\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"DataProcessing\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2648f19-2348-4ad3-9280-82e6eebf2395",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def item_least_rating(data):\n",
    "    least_rated_item = data.orderBy(\"rating\").first().asDict() if data.count() > 0 else None\n",
    "    return least_rated_item\n",
    "\n",
    "def item_most_rating(data):\n",
    "    most_rated_item = data.orderBy(col(\"rating\").desc()).first().asDict() if data.count() > 0 else None\n",
    "    return most_rated_item\n",
    "\n",
    "def item_longest_review(data):\n",
    "    longest_review_item = data.orderBy(length(\"review_text\").desc()).first().asDict() if data.count() > 0 else None\n",
    "    return longest_review_item\n",
    "\n",
    "def transform_date(data):\n",
    "    data = data.withColumn(\"new_date\", data[\"old_date_col\"].cast(StringType()))\n",
    "    return data\n",
    "\n",
    "def desired_dataframe_operation(data):\n",
    "    avg_rating_per_item = data.groupBy(\"item_id\").agg(avg(\"rating\").alias(\"avg_rating\"))\n",
    "    return avg_rating_per_item.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aaecf163-33de-4037-a71a-ca0c03847919",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "    #--------------------------------------------------------------------------\n",
    "    savefile_path = \"/..../output_data.parquet\"  # Replace  with your desired directory name\n",
    "\n",
    "    # Save DataFrame to Parquet file in Databricks File System (DBFS)\n",
    "    transformed_data.write.mode(\"overwrite\").parquet(savefile_path)\n",
    "\n",
    "    #--------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5732f2f0-9c20-4bf1-8073-8252ed526d8a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def main():\n",
    "    sample_data = spark.read.format(\"json\").load(\"dbfs:/FileStore/shared_uploads/wesleyjohnjayakumar@gmail.com/australian_user_reviews.json\")\n",
    "\n",
    "    least_rated_item = item_least_rating(sample_data)\n",
    "    if least_rated_item:\n",
    "        print(\"Item with the least rating:\")\n",
    "        print(least_rated_item)\n",
    "    else:\n",
    "        print(\"No data found for item with the least rating.\")\n",
    "    \n",
    "    #--------------------------------------------------------------------------\n",
    "\n",
    "    most_rated_item = item_most_rating(sample_data)\n",
    "    if most_rated_item:\n",
    "        print(\"\\nItem with the most rating:\")\n",
    "        print(most_rated_item)\n",
    "    else:\n",
    "        print(\"No data found for item with the most rating.\")\n",
    "    #--------------------------------------------------------------------------\n",
    "\n",
    "    longest_review_item = item_longest_review(sample_data)\n",
    "    if longest_review_item:\n",
    "        print(\"\\nItem with the longest review:\")\n",
    "        print(longest_review_item)\n",
    "    else:\n",
    "        print(\"No data found for item with the longest review.\")\n",
    "    #--------------------------------------------------------------------------\n",
    "\n",
    "    transformed_data = transform_date(sample_data)\n",
    "\n",
    "    desired_operation_result = desired_dataframe_operation(transformed_data)\n",
    "    if desired_operation_result:\n",
    "        print(\"\\nDesired DataFrame operation result: Average Rating per Item\")\n",
    "        for row in desired_operation_result:\n",
    "            print(row)\n",
    "    else:\n",
    "        print(\"No data found for desired DataFrame operation.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6d959517-a1b0-4c91-9fd7-dbd258075f2b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"ParquetToMySQL\").getOrCreate()\n",
    "\n",
    "# Read the Parquet file into a PySpark DataFrame\n",
    "file_path = \"path_to_your_parquet_file\"  # Replace with the actual file path\n",
    "loaded_df = spark.read.parquet(file_path)\n",
    "\n",
    "# Display the loaded DataFrame\n",
    "loaded_df.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "33b79772-d90b-45c2-9b3c-9fc7902bfb34",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Define the MySQL properties\n",
    "db_name = 'mydb'\n",
    "host = 'localhost'\n",
    "user = 'root'\n",
    "password = 'password'\n",
    "table_name = 'table_name'\n",
    "\n",
    "# Write the DataFrame to the MySQL table\n",
    "loaded_df.write.format(\"jdbc\").option(\"url\", f\"jdbc:mysql://{host}/{db_name}\") \\\n",
    "    .option(\"dbtable\", table_name) \\\n",
    "    .option(\"user\", user) \\\n",
    "    .option(\"password\", password) \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()\n",
    "\n",
    "# Stop the SparkSession\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Databricks_pipeline",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
